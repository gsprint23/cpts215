{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CptS 215 Data Analytics Systems and Algorithms](https://piazza.com/wsu/fall2017/cpts215/home)\n",
    "[Washington State University](https://wsu.edu)\n",
    "\n",
    "[Gina Sprint](http://eecs.wsu.edu/~gsprint/)\n",
    "## MA6 Heaps and Hierarchical Clustering (50 pts)\n",
    "<mark>Due:</mark>\n",
    "\n",
    "### Learner Objectives\n",
    "At the conclusion of this micro assignment, participants should be able to:\n",
    "* Implement a priority queue using a binary heap\n",
    "* Implement hierarchical clustering using a priority queue\n",
    "\n",
    "### Prerequisites\n",
    "Before starting this micro assignment, participants should be able to:\n",
    "* Write Markdown and code cells in Jupyter Notebook\n",
    "* Implement k-means clustering\n",
    "\n",
    "### Acknowledgments\n",
    "Content used in this assignment is based upon information in the following sources:\n",
    "* [k-Means Clustering](https://www.engage-csedu.org/find-resources/k-means-clustering) assignment by Chris Bailey-Kellogg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Overview and Requirements\n",
    "For this micro assignment, we are going to add functionality to a previous programming assignment ([PA2](http://nbviewer.jupyter.org/github/gsprint23/cpts215/blob/master/progassignments/PA2.ipynb)) where we implemented the k-means clustering algorithm. Please re-read the specifications for the k-means assignment and review your code.\n",
    "\n",
    "### Hierarchical Clustering\n",
    "A problem with k-means clustering is that we may not know what k is (though we could try several and compare the resulting cluster quality). Another way to cluster, avoiding such a parameter (but with its own issues), is called hierarchical clustering. A hierarchical clustering is a binary tree, with higher-up branches connecting subtrees that are less similar, and lower-down branches connecting subtrees that are more similar.\n",
    "\n",
    "For example, consider the following data ([simple.csv](https://raw.githubusercontent.com/gsprint23/cpts215/master/progassignments/files/simple.csv)):\n",
    "\n",
    "|Sample|\n",
    "|-|-|-|-|\n",
    "|g0|0  |0.1|0.2|0  |0.4|0.5|0.6|0.7|0.8|0.9|\n",
    "|g1|1.0|0.9|0.8|0.7|0.6|0.5|0.4|0.3|0  |0.1|\n",
    "|g2|0.1|0.2|0.3|0.4|0.5|0.6|0.7|0.8|0.9|1.0|\n",
    "|g3|0.4|0.4|0.4|0.4|0.4|0.4|0.4|0.4|0.4|0.4|\n",
    "|g4|0.9|0.8|0.7|0.6|0.5|0.4|0.3|0.2|0.1|0.0|\n",
    "|g5|0.5|0  |0.5|0.5|0.5|0.5|0.5|0.5|0.5|0.5|\n",
    "\n",
    "One possible hierarchical clustering for this simple example is as follows (a string representation of the binary tree with \"*\" as the label for the inner nodes):\n",
    "\n",
    "```\n",
    "\t\tg1\n",
    "\t*\n",
    "\t\tg4\n",
    "*\n",
    "\t\t\tg0\n",
    "\t\t*\n",
    "\t\t\tg2\n",
    "\t*\n",
    "\t\t\tg3\n",
    "\t\t*\n",
    "\t\t\tg5\n",
    "```\n",
    "\n",
    "Graphically:\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/cpts215/master/microassignments/figures/simple_clustering_tree.png\" width=600/>\n",
    "\n",
    "The simple made-up case for initial testing is [simple.csv](https://raw.githubusercontent.com/gsprint23/cpts215/master/progassignments/files/simple.csv); the real dataset is [cancer.csv](https://raw.githubusercontent.com/gsprint23/cpts215/master/progassignments/files/cancer.csv).\n",
    "\n",
    "Hierarchical clustering builds a tree bottom up. Each object starts in its own cluster (a leaf node, denoted with a square in the above diagram). We then find the closest pair of clusters, and make them the children of a newly formed cluster (inner node, denoted with a diamond in the above diagram). That cluster is then treated like the rest of the clusters (leaves and inner nodes). We repeat the process: \n",
    "1. Find the closest pair of clusters (which could be leaves or inner nodes)\n",
    "    1. Make them children of a new cluster (inner node)\n",
    "1. Repeat until all that's left is a single object (the root)\n",
    "\n",
    "When we form a cluster, we need to be able to compute its distance to the other clusters (leaves and inner nodes), so that we can determine which pair is closest. Distances involving one or two clusters can be computed in a number of ways. For this assignment we will simply use the centroid of a cluster as a representative point for computing the cluster's distance. We define the centroid of a cluster to be the weighted average of the centroids of all leaves in the subtree.\n",
    "\n",
    "### Implementation\n",
    "We need to keep track of two things: the clusters that need to be further clustered (starting with the leaves), and the distances between them. \n",
    "\n",
    "#### Classes to Define\n",
    "* `Pair`: keeps track of distances between a pair of clusters\n",
    "    * Includes a `__lt__()` for comparing `Pair` objects for heap ordering purposes\n",
    "    * Computes the weighted centroid of cluster pair. This can be done by weighting the average of the centroids of the two clusters by the number of leaves.\n",
    "* `HierarchicalCluster`: a binary tree representing the hierarchical clusters\n",
    "    * Inherits from `BinaryTree`\n",
    "    * `root_data` is the weighted centroid of the tree\n",
    "* `BinaryMinHeap`: a priority queue used to find the closest pair of clusters\n",
    "    * Utilizes `Pair`'s `__lt__()`, that enables us to find the closest pair of clusters, which is the next to merge in the hierarchical clustering\n",
    "    \n",
    "#### Algorithm\n",
    "Initialize the priority queue (`BinaryMinHeap`) with all pairs (`Pair`) of trees. Then when a new cluster is formed, create new pairs between it and the remaining clusters (not its children!) and add them to the queue. Be careful not to merge a cluster twice. An easy way to do this is to test, after removing the closests pair from the priority queue, whether its left and right trees are both still available. That's one reason why as mentioned above we keep track of the clusters that haven't yet been clustered. So be sure to update that list appropriately upon merging. Loop until there's only 1 thing left in the queue â€” the single cluster that's the whole tree (root node).\n",
    "\n",
    "### Observations\n",
    "Test on the two provided datasets, along with any others you like. Note that in addition to the visual display (heatmaps), the tree structure and k-means cluster memberships are printed to the console; paste into a text editor to see everything. (The visual display doesn't capture the tree structure, but does reorder the leaves along the fringe, so you can kind of see patterns of similarity.) Save the textual representations and take screenshots. \n",
    "\n",
    "Write a short (at least a couple paragraphs) description of what you observe in the clusters, e.g. similarities and differences between the two methods and their strengths and weaknesses. Also discuss how well the clustering captures the similarities and differences in the samples (e.g. are same-type cancers clustered?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (5 pts)\n",
    "Do a divisive clusters rather than an agglomerative one; i.e., build the tree from root to leaves, by splitting rather than merging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Assignments\n",
    "1.\tUse the Blackboard tool https://learn.wsu.edu to submit your assignment. You will submit your code to the corresponding programming assignment under the \"Content\" tab. You must upload your solutions as `<your last name>_ma6.zip` by the due date and time.\n",
    "2.\tYour .zip file should contain your .py file(s), .png file(s), and a typed write up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Guidelines\n",
    "This assignment is worth 50 points + 5 points bonus. Your assignment will be evaluated based on a successful compilation and adherence to the program requirements. We will grade according to the following criteria:\n",
    "* 5 pts for correct `Pair` class\n",
    "* 5 pts for correct `HierarchicalCluster` class\n",
    "* 5 pts for using your own heap (`BinaryMinHeap` class)\n",
    "* 5 pts for initial leaf clusters and priority queue\n",
    "* 5 pts for repeatedly creating new clusters from closest unmerged pair\n",
    "* 5 pts for creating new pairs\n",
    "* 5 pts for computing the weighted cluster prototype (centroid)\n",
    "* 5 pts for visualizing the tree (one or more of: text output, graphical output, heatmap output)\n",
    "* 5 pts for including a discussion of clustering patterns, similarities and differences, strengths and weaknesses\n",
    "* 5 pts for adherence to proper programming style and comments established for the class"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
