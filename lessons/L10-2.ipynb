{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CptS 215 Data Analytics Systems and Algorithms](https://piazza.com/wsu/fall2017/cpts215/home)\n",
    "[Washington State University](https://wsu.edu)\n",
    "\n",
    "[Gina Sprint](http://eecs.wsu.edu/~gsprint/)\n",
    "# L10-2 Decision Trees\n",
    "\n",
    "Learner objectives for this lesson:\n",
    "* Understand a decision tree\n",
    "* Understand entropy\n",
    "* Implement the id3 tree building algorithm\n",
    "\n",
    "\n",
    "## Acknowledgments\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* [Data Science from Scratch](https://www.amazon.com/Data-Science-Scratch-Principles-Python/dp/149190142X/ref=sr_1_1?ie=UTF8&qid=1491521130&sr=8-1&keywords=joel+grus) by Joel Grus\n",
    "* [Data Mining](https://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0123748569/ref=sr_1_2?ie=UTF8&qid=1491521294&sr=8-2&keywords=data+mining+ian) by Ian Witten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "A decision tree uses a tree to represent a number of possible *decision paths* and an outcome for each path. Decision trees are a popular machine learning model because they are easy to read and interpret, the decision path for unseen data is transparent, and when used in more advanced machine learning algorithms (random forests, ensemble learning, etc.) they form quite powerful predictive models. Decision trees can be trained to predict categorical outcomes (classification trees) or numeric outcomes (regression trees). We are going to focus on *binary* classification trees, trees that predict one of two possible categorical outcomes. For example, predicting \"hire\" or \"no hire\" for potential job candidates is an example of a binary classification problem.\n",
    "\n",
    "Non-leaf nodes (decision nodes) in a decision tree involve testing a particular feature, or attribute, typically against a constant value. Leaf nodes in a decision tree represent a classification, a set of classifications, or a probability distribution over all possible classifications that applies to all instances that reach the leaf.\n",
    "\n",
    "Let's take a look at an example. Suppose we have built a tree to classify whether a passenger aboard the [Titanic](https://en.wikipedia.org/wiki/RMS_Titanic) survived the shipwreck or not:\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png\" width=\"400\"/>\n",
    "\n",
    "(image from [https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png))\n",
    "\n",
    "Where \"sex\" is the gender of the passenger, \"age\" is the age of the passenger in years (fractional if age is less than one), and \"sibsp\" is the number of siblings/spouses aboard the vessel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through a few examples of making a prediction for different passengers using this tree. To classify an example passenger as \"survived\" or \"died\" the example is routed down the tree according to the values of its attributes tested in successive nodes. When a leaf node is reached the example is classified based on the class assigned to the leaf.\n",
    "\n",
    "|sex|age|sibsp|prediction|\n",
    "|-|-|-|-|\n",
    "|female|30|0|survived|\n",
    "|male|45|2|died|\n",
    "|male|8|0|survived|\n",
    "|male|6|3|died|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Tree Implementation\n",
    "To implement a classification tree, we are going to have to decide what questions to ask (and in what order) to partition our dataset into a decision tree structure. To do this, we need to find out which features are the most informative. A feature that clearly divides the examples is a good feature to use in our tree. \n",
    "\n",
    "### Entropy\n",
    "Let's first discuss the notion of *entropy*. Entropy is going to be our measure of how much uncertainty is contained in the data. If all examples in a dataset belong to the same class, then there is no uncertainty about which class these examples belong to and therefore low entropy. If the examples in a dataset are evenly spread across different classes, then there is a high amount of uncertainty and therefore high entropy.\n",
    "\n",
    "(notation adopted from Joel Grus)\n",
    "\n",
    "To code up a function to compute the entropy of a data set, we need a mathematical definition of entropy. Let $S$ by a dataset contain examples that each belong to one of a finite number ($n$) of classes $C_{1},...,C_{n}$. If $P_{i}$ is the proportion of data labeled as class $C_{i}$, then entropy $H$ of the dataset $S$ is defined as:\n",
    "\n",
    "$$H(S) = -p_{1} log_{2} p_{1} - ... - p_{n} log_{2} p_{n}$$\n",
    "\n",
    "such that $0 log 0 = 0$. Each term $-p_{i} log_{2} p_{i}$ is close to zero when $p_{i}$ is either close to zero or close to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x6c87ebca58>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEQCAYAAABMXyhMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xuc1nPex/HXpyNh2yRFbTksshEVKcIoElbdsqiQrGhz\n2LZ1il237Np13INYWZYkUjZJUoTbRA4dKDlU2HRep0SUapo+9x/fSWNMdc3VdV3f6/B+Ph7zaK6Z\n3/yut5+Z63P9vkdzd0RERBJRLXYAERHJHSoaIiKSMBUNERFJmIqGiIgkTEVDREQSpqIhIiIJi1I0\nzKyLmc0zs/fN7OotHFNkZrPM7B0zezHTGUVE5Ics0/M0zKwa8D7QCVgOzAB6uPu8csfUBV4FOrv7\nMjPbzd0/z2hQERH5gRh3Gm2BD9x9kbuXAKOAbhWO6QU87u7LAFQwRESyQ4yi0RhYUu7x0rKvlbc/\nsKuZvWhmM8zs3IylExGRLaoRO8AW1ABaAx2BnYDXzOw1d/8wbiwRkcIWo2gsA5qWe9yk7GvlLQU+\nd/e1wFozewk4BPhe0TAzLZwlIpIEd7dkfi5G89QM4Kdm1szMagE9gPEVjnkS6GBm1c2sDnAEMLey\nk7m7Pty5/vrro2fIlg9dC10LXYutf2yPjN9puHupmV0KTCYUrfvdfa6Z9Qvf9nvdfZ6ZPQvMAUqB\ne939vUxnFRGR74vSp+HuzwAHVPjaPys8vh24PZO5RERk6zQjPE8UFRXFjpA1dC0207XYTNciNTI+\nuS+VzMxzOb+ISAxmhudQR7iIiOQoFQ0REUmYioaIiCRMRUNERBKmoiEiIglT0RARkYSpaIiISMJU\nNEREJGEqGiIikjAVDRERSZiKhoiIJExFQ0REEqaiISIiCVPREBGRhKloiIhIwlQ0REQkYSoaIiKS\nMBUNERFJWI3YAUQqU1oKq1bBt9+Gz0tLwR122OH7H5bUhpUikiwVDcm40lJYuBDmz4f334f//AeW\nLYPly8PHypWwZg3svDPsuCPUqAHVq4cCsW4drF0biok77Lor1K8PDRvCT34SPpo1g/33hwMOgN13\nV2ERSSVz99gZkmZmnsv5C8XChfDSSzBjBrzxBsyZA7vtFl7U998f9t0XmjSBxo1hjz1CIdh5Z6i2\njcbTb7+FL74IHx9/DEuWwOLFsGhRKEjz58PGjdCyJbRqBa1bw2GHQfPm2z63SD4zM9w9qbdTKhqS\ncitXwjPPhI8pU8KdwbHHQtu20KZNeAGvWzczWT77DN56C958M3xMnw5ffQVHHgkdOkCnTiFP9eqZ\nySOSDVQ0JLpPPoHHHoOxY8PdxLHHwsknQ8eO4W4im5qI/vtfeOWVcPfz/PPw6ach58knwymnQIMG\nsROKpJeKhkTx7bcwZgw8/HB4B3/qqfCLX8Dxx0OdOrHTJW7ZMnjuOXj66fDvQQfBaafBGWdA06ax\n04mknoqGZNT8+XDPPTBiRGhyOu+8UDByqVBsybp18OKL8Pjj8MQT4S6pRw/o2VN3IJI/VDQk7dxh\n6lS4+WaYORN++Uu46CLYe+/YydKnpCQ0X40cCU89BccdB336hGasmjVjpxNJXs4VDTPrAvydMLnw\nfne/pcL3jwWeBBaUfWmsu99YyXlUNNLMHSZOhD/9KXQqX3kl9O4d5kgUklWrQlPcsGFhiHDfvnDh\nhWGIr0iuyamiYWbVgPeBTsByYAbQw93nlTvmWOByd++6jXOpaKTRlClw7bVhtNH110P37hplBPDu\nu6F57pFHQof/wIFw9NHZ1dkvsjXbUzRijFZvC3zg7ovcvQQYBXSr5Dj9CUby7rvQpQucfz707x+G\nrJ5xhgrGJi1awJ13hjkhnTuHO47DDgtFpKQkdjqR9IpRNBoDS8o9Xlr2tYram9lsM3vazH6WmWiF\n7csvw7vmoiI46SSYNw/OOUfFYkt23jkU1blz4YYb4F//Ch3nQ4eGuSki+Shb58W+ATR190OBu4Bx\nkfPkNffwLvnAA+Gbb+C992DAAKhVK3ay3FCtGvz852HU1ciRMGlSGCDwl7+E5VBE8kmMtaeWAeVH\nvzcp+9p33P2bcp9PMrO7zWxXd/+i4skGDx783edFRUUUFRWlOm9eW7IkvFtevBiefDIMoZXktW8P\n48eHJr0//CEUjkGDwkizQhs8INmjuLiY4uLilJwrRkd4dWA+oSP8v8B0oKe7zy13TEN3/6Ts87bA\nY+6+VyXnUkd4ktzh/vvhmmvgssvCC5vuLFJv9uwwiODNN0MTVu/eYQFGkZhyavQUfDfk9g42D7m9\n2cz6Ae7u95rZJUB/oAT4Fhjo7tMqOY+KRhJWrAidtwsWhNncBx0UO1H+e/11uPpq+Pxz+POfoWtX\njbaSeHKuaKSKikbVvfhieLd7xhlw001Qu3bsRIXDPfR3XHVVmF3+t7/BoYfGTiWFKNeG3EoEGzeG\nd7i9esF998Ff/6qCkWlmYTb57Nlw1llhWHPfvmGxR5FcoaJRAFatgtNPD0thzJwZXqwknho14Fe/\nCkOa69YNzYN33gkbNsROJrJtKhp5bt48OPxwaNQIiovDRkeSHX784zC6asqUsDhimzZhyXaRbKai\nkceKi8MyF1deGSacqTkqO/3sZ/DCC2HJljPPhH79wkRLkWykopGnhg8PL0CPPhrazSW7mYV+jvfe\nCzPwW7QIm1ppnIdkG42eyjPuYVLZ8OEwYUJ4Fyu559VXw7Do/fYLd4l77BE7keQTjZ4SIIyQGjAA\nxo2D115TwchlRx4ZJgQefDAcckh4E6D3R5INdKeRJzZsgAsuCHs9TJgQOlklP8yaFTa9atw4LIrY\nqFHsRJLrdKdR4NavD/0Xn3wCkyerYOSbVq1g2jRo3TpMBhwzJnYiKWS608hxJSVhdrcZjB6t9aPy\n3bRpcO65YWHJf/wjzPMQqSrdaRSokhLo2TP0ZahgFIYjjgjNVTvvHO46Xn01diIpNLrTyFEbNoQN\nkr7+GsaO1RyMQjRuXJjTcfHF8LvfafVcSZwWLCww7qFjdNmysHeD9mkoXMuXhwUoS0rCBlCa8S+J\nUPNUgbnmmrDF6BNPqGAUuj33hGefhRNOCMuQPPNM7ESS73SnkWP+9je4916YOhXq14+dRrLJlClw\n9tmho/yPf1RzlWyZmqcKxCOPhLuMqVOhadNtHy+F57PPwvL3paVhCZmGDWMnkmyk5qkC8PLLMHBg\n2MRHBUO2pEGD0ETVoUNorpo6NXYiyTe608gBCxbAUUeFpSQ6d46dRnLFpEnQpw9cdx1ccom2l5XN\n1DyVx776KqxDdPHF4Q9fpCoWLIDTTguzyocOhR13jJ1IsoGap/JUaSn06AFFRSoYkpx99gkTANet\ng6OPhiVLYieSXKeikcWuuy6sK3XHHbGTSC7baacwh+Oss8KM8tdei51Icpmap7LUk0/CZZfBG2+E\nzk2RVJg4MfRz3Hpr+FcKk/o08swHH4SO76eeCu8MRVJp7lzo2hW6dYNbbgk7BUphUdHII6tXQ/v2\n8Ktfhc5vkXT44gvo3j0so//II6EJSwqHOsLzyGWXhZ3a+vePnUTy2a67hr1X6tWDY44J65iJJEJF\nI4uMHh0mYw0dqjH1kn61asEDD4T9WNq3h7feip1IcoGap7LEwoVhY52JE+Gww2KnkUIzenS4y334\nYU0gLQRqnspxGzaEheauvFIFQ+I46yx4/PGwzPqwYbHTSDbTOphZ4MYboU4duPzy2EmkkB19dFgp\n9+STwyTA665TM6n8UJQ7DTPrYmbzzOx9M7t6K8cdbmYlZtY9k/kyaebM0IcxfDhU032fRHbAAfDK\nK2FXwIsvDqsSiJSX8ZcpM6sG3AWcCLQAeppZ8y0cdzPwbGYTZs66dXDeeWGPjD33jJ1GJGjUCIqL\nw3yhM86AtWtjJ5JsEuO9bVvgA3df5O4lwCigWyXHXQaMAT7NZLhMGjw4vLPr2TN2EpHv+9GPwqCM\n2rXhxBPDwpkiEKdoNAbKL5u2tOxr3zGzPYH/cfehQF62qk6fHoY7anitZKtatcLEv5Ytw6KZn3wS\nO5Fkg2ztCP87UL6vY4svq4MHD/7u86KiIoqKitIWKlXWrg3r/gwZop3VJLtVqxZ+T2+4IXSUT54M\ne+0VO5VUVXFxMcXFxSk5V8bnaZhZO2Cwu3cpezwIcHe/pdwxCzZ9CuwGrAYucvfxFc6Vk/M0rr8e\n5syBsWN1lyG5Y8gQuO22UDgOPDB2GtkeObX2lJlVB+YDnYD/AtOBnu4+dwvHDwOecvexlXwv54rG\n/PlhMcLZs6FJk9hpRKrmoYfg6qtDf0erVrHTSLK2p2hkvHnK3UvN7FJgMqFP5X53n2tm/cK3/d6K\nP5LpjOniHhYivO46FQzJTb17h8UNu3QJw3Lbt4+dSDJNy4hk0EMPhQ2Vpk/XctSS2yZNCgXkscfg\nuONip5GqyqnmqVTKpaKxYgW0aAFPPw1t2sROI7L9iovhzDNhxIgwLFdyh4pGDujXL4x5HzIkdhKR\n1HnlFTjttDB8/Oc/j51GEqWikeVmzw5twPPmhU1vRPLJ9Olw6qlwzz2hgEj2y6mO8ELjDgMGhHHu\nKhiSj9q2DX0cJ50EGzfC6afHTiTppKKRZmPGwJdfQt++sZOIpE/r1vDMM6FwgApHPlPRSKNvvw17\nZDz4oEZLSf5r1SoUji5dwmMVjvykopFGt98Ohx8e1u0RKQSHHrq5cJhB97zd1KBwqSM8TT7+OAyx\nnTkT9t47dhqRzHrzzdBUdd990LVr7DRSkUZPZaGLL4YddoC//jV2EpE4ZsyAU04JzbMnnxw7jZSn\nopFlPvwQ2rULQ2x32y12GpF4Xn893Gk88giccELsNLLJ9hQNbTCaBr//PQwcqIIh0q5dWM357LPh\npZdip5FU0J1Gir3xRpjo9MEHYWE3EYEXXgg7VD71FBxxROw0ojuNLDJoUFjFVgVDZLNOnWDYsNBU\nNXt27DSyPVQ0UujFF+GjjzSRT6Qyp5wCd98dOsXnz4+dRpKleRopNHgw/O//Qs2asZOIZKfTT4ev\nv4bOneHll6Fp09iJpKpUNFKkuBiWL4devWInEcluffrAV1/B8ceHwtGwYexEUhUqGilyww1h1FQN\nXVGRbRowIBSOzp3DG6569WInkkQlNHrKzGoAZwCbNnfcCSgF1gBzgJHuvjZdIbeSKytGT02ZAhdc\nEOZlqGiIJMYdfvvbMAlw8mSoUyd2osKR1sl9ZnY4cDTwnLu/Xcn39wVOAd5y9ynJhEhWthSNjh3h\n3HPh/PNjJxHJLRs3wnnnwcqV8MQT6g/MlHQXjYMrKxaVHLcPsNTd1ycTJBnZUDReeim00c6fr194\nkWSUlITNm+rVg+HDoZrGdKZdWudplC8YZnahmf2fmb1sZhdVOG5BJgtGtvjzn+Haa1UwRJJVsyY8\n9hgsXAhXXBGarSR7VbWmr3D3jkBXYJ2ZDUpDppzx1lswZ05omhKR5NWpA+PHh76N22+PnUa2pqpF\nYwcza+3uK919OPBuOkLlittuC6NAateOnUQk99WrF/biuOsuGDEidhrZkiqtPWVmNxOG6bYAHFgP\n/B34ibtn/H9zzD6NRYvCTmULFmjvb5FUeu89OO640L+xaRdASa2MLY1uZu3LfuZVM6sFHAYcCfRy\n99bJBNgeMYvGwIFheO1tt0V5epG89uqr0K1buPNo0yZ2mvyTtqJhZrWBnd19xTYC7AOUuPuSZEIk\nK1bR+OIL+OlPQ39GkyYZf3qRgvDEE3DppTB1qna/TLXtKRpbnYrm7uvM7AQz2wUY5+7fVvLkPwaO\nB94DMlo0Yhk6NLwLUsEQSZ/TTgtL85x0ErzyCtSvHzuRQOIzwhsBvwR2B3YgFJtNM8KXAv9y96/S\nmHNLuTJ+p7FuHTRrFvYHaNEio08tUpAGDQprVD3/POy4Y+w0+SHntns1sy6EDvRqwP3ufkuF73cF\n/ghsBEqAge7+SiXnyXjRGDEifEyenNGnFSlYGzeGYe3r18Po0Zr8lwoZLxpmtpO7ry5bk2qju2+s\nws9WA94HOgHLgRlAD3efV+6YOu6+puzzg4HH3P3ASs6V8aLRtm3YZOnUUzP6tCIFbd26sMd4u3Zw\n662x0+S+jO7cZ2ZXAdeb2e1AXeCeKp6iLfCBuy9y9xJgFNCt/AGbCkaZnQl3HNFNmwaffx42kRGR\nzKldG8aNCxMAhw6NnaawJbMm6zTgdUKz0S+oeuFpzPc7zJcSCsn3mNn/ADcBDQgLIkZ3551wySVQ\nvXrsJCKFZ9ddYeJEOOqosHnTKVnxqlB4kika3wB93P2fwGNlTVQp5+7jgHFm1gG4ETihsuMGDx78\n3edFRUUUFRWlIw4ffwxPPx0Kh4jEsc8+YShu167w3HNwyCGxE+WG4uJiiouLU3Kuqk7uu4/QVPQK\nMNXdF1T5Cc3aAYPdvUvZ40GAV+wMr/Az/wEOd/cvKnw9Y30af/gDLFsG//xnRp5ORLZi9Gi48srQ\nZLzHHrHT5J5M9mlMBW4AVgG/NrOZZjbMzKqy0+8M4Kdm1qxsVnkPYHz5A8r26Nj0eWugVsWCkUnr\n18M994SJRiIS31lnQb9+YUDK6tWx0xSWqhaNnwCr3H2cu/+G0OcwAEh4Z2x3LwUuBSYTFjwc5e5z\nzaxfueXWTzezd8zsTeBO4Mwq5kypJ5+E/feHgw+OmUJEyrv22jBX6txzw7BcyYyqNk/twebRUvOB\nUne/xsy6ufuT6Qi4jTwZaZ468cSwu1ivhEujiGTCpqG4HTqEvW0kMTHmaTQD6gFvA/WBW9w945ud\nZqJoLFwIhx0GS5fCDjuk9alEJAmffQZHHAE33KC9bRKVtrWnKnmiA4H+wJfAiLKmpk+BvN0d+4EH\nwh2GCoZIdmrQAJ56Kiynvu++cOSRsRPlt6o2T10OTAKaAmcAY9x9UpqyJZInrXcapaVhnamJE6Fl\ny7Q9jYikwKRJcMEF8Npr4e9WtiyTo6c+d/f33P0Zd7+AMPEubz37LDRurIIhkgtOOikMw+3aFb75\nJnaa/FXlomFmj5rZqWbWEmiYjlDZ4r77oG/f2ClEJFG/+U3YtOm88zSiKl2q3BFuZvsD5wG1gPvc\n/f10BEswS9qapz7+GA48EBYvhl12SctTiEgarFsHHTuGUVXlFoyQctLaPGVmfzSz08r21MDd33f3\n3wETgM+SedJcMHw4nH66CoZIrqldG8aOhWHDYMyY2GnyTyLNUzsCuwI3mtkkMxtpZgMICxZekNZ0\nkbiHotGnT+wkIpKMhg3DGlX9+4dtmSV1kmmeqgu0I6xM+5G7P5yOYAlmSUvz1KxZ0L07LFgAltQN\nnIhkg0cfDTPHZ8yA3XaLnSZ7ZGz0lJldCDwB/B74JGbBSKeHH4ZzzlHBEMl1PXvCGWfAmWdCSUns\nNPmhqqOnVrh7R6ArsK5shdq8Uloa3p2cfXbsJCKSCjfdFPo5rrgidpL8UNWisYOZtXb3le4+nLDg\nYF75v/8LczOaN4+dRERSoXp1GDkyTNJ96KHYaXJfVTdQagm0NrM/AQ6sN7OvgZ+4+4iUp4tgU9OU\niOSPevVCx/hxx4WVcdu0iZ0od1V1GZH2ZT/zatleGIcBRwK93L11mjJuLU9KO8JXr4YmTWDevDD6\nQkTyy5gxoZlqxoywZlWhyvgqt5UE2CeZXfxS8LwpLRqPPhpuXydFW01LRNLtmmvCjn+TJ0ONtGxW\nnf0yufZUpWIUjHQYMUJLK4vkuxtvhJo1Q/GQqkvJnUYsqbzTWLEibFq/fDnstFNKTikiWWrFirBP\nzm23wS9+ETtN5kW/08gHTz4JnTurYIgUgvr14fHHw4zxuXNjp8ktKhplxowpzHccIoWqdWu49VY4\n7TRYtSp2mtyh5ilg5UrYa6+wpasWKBQpLP36heaqf/+7cFaBUPPUdtq0VaQKhkjhueMOWLQI/va3\n2ElyQ4EOOPu+MWPC2jQiUnh22CHcZRxxBBx+OBx9dOxE2a3gm6dWrQoT+pYsgbp1UxRMRHLOM8+E\nnTpnzoRGjWKnSS81T22HCRPgmGNUMEQKXZcuoWj06AEbNsROk70Kvmho1JSIbHLddWFF3Ouui50k\nexV089Q338Cee8LChbDrrqnLJSK567PPwnDcoUPh5z+PnSY91DyVpEmToH17FQwR2axBAxg1Ci64\nIIyqku8r6KIxYQJ07Ro7hYhkm6OOgiuvDKMq162LnSa7RCkaZtbFzOaZ2ftmdnUl3+9lZm+VfUw1\ns4NTnaG0NNxpnHJKqs8sIvng8sthjz3g6h+8QhW2jBcNM6sG3AWcCLQAeppZxX3yFgDHuPshwI3A\nfanOMWNG2DNjr71SfWYRyQdmMGxYWJfuiSdip8keMe402gIfuPsidy8BRgHdyh/g7q+7+1dlD18H\nGqc6xIQJussQka2rVw9Gjw5LjXz0Uew02SFG0WgMLCn3eClbLwp9gZRvizRhQv6OjBCR1GnbFq69\nFs46C9avj50mvqxeRsTMjgPOBzps6ZjBgwd/93lRURFFRUXbPO+SJeGjXbvtzygi+W/AAJgyBa66\nCv7+99hpqq64uJji4uKUnCvj8zTMrB0w2N27lD0eBLi731LhuJbA40AXd//PFs6V1DyNe+6BqVPh\n4Yer/KMiUqBWroRWrWDIkNwfdZlr8zRmAD81s2ZmVgvoAYwvf4CZNSUUjHO3VDC2h5qmRKSq6tWD\nRx+FCy+ExYtjp4knyoxwM+sC3EEoWve7+81m1o9wx3Gvmd0HdAcWAQaUuHvbSs5T5TuNNWvCqKnF\ni8MvgYhIVdxyC4wfD8XFYa/xXLQ9dxoFt4zIhAlw++3hf7iISFVt3AgnnxyWGvnzn2OnSU6uNU9F\n9fTTGmorIsmrVg0eegiGD4fnn4+dJvMKrmg89xyceGLsFCKSy3bfPRSO886DTz+NnSazCqpofPQR\nfP01HHRQ7CQikus6dYLevaFPn9BkVSgKqmi88EL4H12toP6rRSRd/vCHMBT3jjtiJ8mcrJ7cl2ov\nvAAnnBA7hYjki5o1YeTIsL/4McdAmzaxE6Vfwbzn3rgxFI3jj4+dRETyyd57hwl/PXuGjd3yXcEU\njbffDvuAN20aO4mI5JsePcIeHL/5Tewk6VcwReP553WXISLpM2RIWJ/q3/+OnSS9VDRERFJgl13C\nMiOXXJLf28QWxIzw9etht91g4ULtBy4i6XXrrfDUU2HVierVY6epnGaEb8Prr8P++6tgiEj6XXEF\n1KoFN98cO0l6FETRUNOUiGRKtWphiZEhQ2D69NhpUk9FQ0QkxZo0gbvugrPPzr9huHnfp/HNN2Ep\n9M8/hx13zFAwERHgl78M/z7wQNwcFalPYyumTYNDDlHBEJHMu+MOeOklGDs2dpLUyfui8cor0GGL\nO4yLiKTPLrvAiBHQvz8sXx47TWoURNE46qjYKUSkULVvD7/6VWiqyuHegO/kdZ9GaWkYZvvhh9Cg\nQQaDiYiUU1ISWjzOPRcuvTR2mu3r08jrVW7feQcaNVLBEJG4ataEhx+GI48M2zMceGDsRMnL6+ap\nqVPVnyEi2WG//eDGG+Gcc8IqFbkqr4uG+jNEJJtcdFFo/fjjH2MnSV5e92k0awaTJ8MBB2QwlIjI\nVnz8MRx6KIwbB+3axcmgeRqVWLIE1qwJa06JiGSLRo3g7rtDp/jq1bHTVF3eFo1NTVOWVC0VEUmf\n7t1Dp/jll8dOUnV5XzRERLLRkCEwaRI880zsJFWjoiEiEkHdujBsGPTtCytXxk6TuLzsCP/669Bu\n+MUXULt2hGAiIgn69a9hxQp45JHMPac6wiuYNg1atVLBEJHsd/PNMHNm7uwtHqVomFkXM5tnZu+b\n2dWVfP8AM3vVzNaa2W+rev6ZM6Ft29RkFRFJpzp14KGHwvIiH38cO822ZbxomFk14C7gRKAF0NPM\nmlc4bAVwGXBbMs/x5pvQuvV2xRQRyZgjjoALLoB+/bJ/UcMYdxptgQ/cfZG7lwCjgG7lD3D3z939\nDWBDMk8wa5aKhojkluuvh48+CkupZ7MYRaMxsKTc46VlX0uJr74K69ZrFriI5JLatcPe4ldcAUuX\nxk6zZXnXET57NrRsCdWrx04iIlI1rVqFvo2+fbO3mSrG0ujLgKblHjcp+1pSBg8e/N3nRUVFzJ5d\nRKtWSWcTEYnqmmvCxk3/+hdceGFqzllcXExxcXFKzpXxeRpmVh2YD3QC/gtMB3q6+9xKjr0e+Mbd\n/7KFc/1gnkbv3nDMMaFSi4jkonfegeOOgzfegKZNt318VeXUPA13LwUuBSYD7wKj3H2umfUzs4sA\nzKyhmS0BBgK/M7PFZrZzIudXJ7iI5LqDDoKBA7OzmSqvZoSvWQP168OXX2pin4jktg0bQjPVhReG\nfThSKafuNNLp7beheXMVDBHJfTVqwIMPwrXXwsKFsdNslldFY9Ys1AkuInmjRYswBDebmqnyqmho\nJriI5JsrroBVq+C++2InCVQ0RESyWI0a8MAD8LvfweLFsdPkUUd4SUlYn/7TT2HnhMZZiYjkjj/9\nCV5+OWzctL07kqojHHjvPWjWTAVDRPLTVVeFN8XDhsXNkTdFQ/MzRCSf1awZRlMNGgTLkl5DY/vl\nTdF4802NnBKR/NayJVx8MfTvH280Vd4UjYMOgk6dYqcQEUmva68NS6iPGhXn+fOmI1xEpFDMmAGn\nngpz5sDuu1f957enI1xFQ0QkB111VRiCm8wdh0ZPiYgUmBtuCH2548Zl9nl1pyEikqNeegl69QpL\nqf/4x4n/nJqnREQKVP/+YUXcqiwzoqIhIlKgVq0Ko0cffBA6dkzsZ9SnISJSoH70I7j77rDnxpo1\n6X8+3WmIiOSBXr1gzz3h9tu3fayap0RECtxnn4VmqokToU2brR+r5ikRkQLXoEG4y+jbN6z6nS4q\nGiIieeKcc8IM8b/+NX3PoeYpEZE88tFHcPjh8NprsN9+lR+j5ikREQFg773DLn8XXZSelXBVNERE\n8syvfw2rV6dnwyY1T4mI5KHZs6FzZ3j7bWjY8Pvf05BbERH5gauvhiVLYOTI739dRUNERH5gzRo4\n+GC46y6UiayVAAAFOElEQVQ46aTNX1dHuIiI/ECdOnDPPWGL2NWrU3NO3WmIiOS53r3D/I1NS4zk\n3J2GmXUxs3lm9r6ZXb2FY4aY2QdmNtvMDs10RhGRfPGXv8CIETBr1vafK+NFw8yqAXcBJwItgJ5m\n1rzCMScB+7r7fkA/4J5M58w1xcXFsSNkDV2LzXQtNivka9GgAdx8c5i7UVq6feeKcafRFvjA3Re5\newkwCuhW4ZhuwEMA7j4NqGtmFQaNSXmF/AdRka7FZroWmxX6tejTB3baCf7xj+07T42UpKmaxsCS\nco+XEgrJ1o5ZVva1T9IbTUQkP5mFTvEOHbbvPBo9JSJSIJo3h0sv3b5zZHz0lJm1Awa7e5eyx4MA\nd/dbyh1zD/Ciu48uezwPONbdP6lwLg2dEhFJQrKjp2I0T80AfmpmzYD/Aj2AnhWOGQ9cAowuKzJf\nViwYkPx/tIiIJCfjRcPdS83sUmAyoXnsfnefa2b9wrf9XnefaGYnm9mHwGrg/EznFBGRH8rpyX0i\nIpJZOdERrsmAm23rWphZLzN7q+xjqpkdHCNnJiTye1F23OFmVmJm3TOZL5MS/BspMrNZZvaOmb2Y\n6YyZksDfyI/MbHzZa8XbZtYnQsy0M7P7zewTM5uzlWOq/rrp7ln9QShsHwLNgJrAbKB5hWNOAp4u\n+/wI4PXYuSNei3ZA3bLPuxTytSh33AvABKB77NwRfy/qAu8Cjcse7xY7d8RrcQ1w06brAKwAasTO\nnoZr0QE4FJizhe8n9bqZC3camgy42Tavhbu/7u5flT18nTC/JR8l8nsBcBkwBvg0k+EyLJFr0Qt4\n3N2XAbj75xnOmCmJXAsHdin7fBdghbtvyGDGjHD3qcDKrRyS1OtmLhSNyiYDVnwh3NJkwHyTyLUo\nry8wKa2J4tnmtTCzPYH/cfehQD6PtEvk92J/YFcze9HMZpjZuRlLl1mJXIu7gJ+Z2XLgLWBAhrJl\nm6ReN2MMuZUMMLPjCKPOtnP+Z077O1C+TTufC8e21ABaAx2BnYDXzOw1d/8wbqwoTgRmuXtHM9sX\neM7MWrr7N7GD5YJcKBrLgKblHjcp+1rFY36yjWPyQSLXAjNrCdwLdHH3rd2e5rJErsVhwCgzM0Lb\n9UlmVuLu4zOUMVMSuRZLgc/dfS2w1sxeAg4htP/nk0SuxfnATQDu/h8z+whoDszMSMLskdTrZi40\nT303GdDMahEmA1b8ox8P9IbvZpxXOhkwD2zzWphZU+Bx4Fx3/0+EjJmyzWvh7vuUfexN6Ne4OA8L\nBiT2N/Ik0MHMqptZHULH59wM58yERK7FIuB4gLI2/P2BBRlNmTnGlu+wk3rdzPo7DddkwO8kci2A\n64BdgbvL3mGXuHvFBSFzXoLX4ns/kvGQGZLg38g8M3sWmAOUAve6+3sRY6dFgr8XNwIPlhuKepW7\nfxEpctqY2UigCKhvZouB64FabOfrpib3iYhIwnKheUpERLKEioaIiCRMRUNERBKmoiEiIglT0RAR\nkYSpaIiISMJUNEREJGEqGiIikrCsnxEukkvMrD1hyek5wFqgvrvfFzeVSOroTkMktYzwZmyeu48F\nekbOI5JSKhoiKeTurwIt3P1NM6tH2D1OJG+oaIikUNnKqpsWdOsKjIgYRyTlVDREUuswYL2ZnQrs\nUclquyI5TavciqSQmV0BzHT34thZRNJBo6dEUsTM9gF6Afm4AZgIoDsNERGpAvVpiIhIwlQ0REQk\nYSoaIiKSMBUNERFJmIqGiIgkTEVDREQSpqIhIiIJU9EQEZGE/T95BeP8wpWpYQAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6c87f144e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "p = np.arange(0.01, 1.01, 0.01)\n",
    "plogp = -p * np.log2(p)\n",
    "plt.xlabel(\"$p$\")\n",
    "plt.ylabel(\"$p log_{2}(p)$\")\n",
    "plt.plot(p, plogp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summation of $H(s)$ will be small when each $p_{i}$ is close to zero or close to one (most examples belong to a single class) and will be large when each $p_{i}$ is close to ~0.5 (the examples are spread across multiple classes). Let's write the code to represent this mathematical behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1  0.1  0.8] 0.921928094887\n",
      "[ 0.3  0.4  0.3] 1.57095059445\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_entropy(class_probabilities):\n",
    "    '''\n",
    "    class_probabilities is a list of class probabilities\n",
    "    '''\n",
    "    terms = [-pi * np.log2(pi) for pi in class_probabilities if pi] # ignore zero probabilities\n",
    "    H = np.sum(terms)\n",
    "    return H\n",
    "\n",
    "def compute_class_probabilities(instance_labels):\n",
    "    '''\n",
    "    instance_labels is a list of each examples' class label\n",
    "    '''\n",
    "    num_examples = len(instance_labels)\n",
    "    counts = list(Counter(instance_labels).values())\n",
    "    probabilities = np.array(counts) / num_examples\n",
    "    return probabilities\n",
    "\n",
    "def compute_subset_entropy(subset):\n",
    "    '''\n",
    "    subset is a list of instances as two-item tuples (attributes, label)\n",
    "    '''\n",
    "    labels = [label for _, label in subset]\n",
    "    probabilities = compute_class_probabilities(labels)\n",
    "    entropy = compute_entropy(probabilities)\n",
    "    return entropy\n",
    "    \n",
    "# all in one class -> low uncertainty -> low entropy\n",
    "x = np.array([5, 5, 5, 5, 5, 1, 5, 2, 5, 5])\n",
    "cps = compute_class_probabilities(x)\n",
    "entropy = compute_entropy(cps)\n",
    "print(cps, entropy)\n",
    "\n",
    "# even spread across classes -> high uncertainty -> high entropy\n",
    "x = np.array([5, 5, 5, 1, 1, 1, 2, 2, 2, 2])\n",
    "cps = compute_class_probabilities(x)\n",
    "entropy = compute_entropy(cps)\n",
    "print(cps, entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to apply our measurement of entropy to build our tree. As we construct our tree, we are going to need to figure out what decision to make at our decision nodes. To do this, we are going to partition our data set $S$ into subsets $S_{1},...,S_{m}$ containing proportions $q_{1},...,q_{m}$ of the data. We can then compute the entropy of the partition as the weighted sum of each subset's entropy:\n",
    "\n",
    "$$H = -q_{1} H(S_{1}) + ... + q_{m} H(S_{m})$$\n",
    "\n",
    "Ideally, we want a partition to have low entropy if it splits the data into subsets that have low entropy and high entropy if it splits the data insto subsets that are large and have high entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_partition_entropy(subsets):\n",
    "    '''\n",
    "    subsets is a list of class label lists\n",
    "    '''\n",
    "    num_examples = np.sum([len(s) for s in subsets])\n",
    "    entropies = [(len(s) / num_examples) * compute_subset_entropy(s) for s in subsets]\n",
    "    partition_entropy = np.sum(entropies)\n",
    "    return partition_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Algorithm\n",
    "Next, we need to add our infrastructure to read in the data, build a tree, and make classifications on unseen data. To build a tree, we are going to implement the ID3 algorithm. Starting with the entire dataset and all attributes, we are going to follow the following process to build a tree (from Joel Grus' text):\n",
    "1. If the data all have the same label, then create a leaf node that predicts that label and then stop.\n",
    "1. If the list of attributes is empty (i.e. there are no more possible questions to ask), then create a leaf node that predicts the most common classification label and then stop.\n",
    "1. Otherwise, try partition the data by each of the attributes.\n",
    "    1. Choose the partition with the lowest partition entropy.\n",
    "    1. Add a decision node based on the chosen attribute.\n",
    "    1. Recur on each partitioned subset using the remaining attributes.\n",
    "    \n",
    "> This is what's known as a \"greedy\" algorithm because, at each step, it chooses the most immediately best option. Given a dataset, there may be a more optimal tree with a worse-looking first move. If so, this algorithm won't find it. Nonetheless, it is relatively easy to understand and implement, which makes it a good place to being exploring decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Dataset\n",
    "To implement the ID3 algorithm step by step, we will use Joel Grus' example dataset. In this dataset, each instance example is an attribute list describing a job candidate:\n",
    "* Level of expertise (string)\n",
    "* Preferred language (string)\n",
    "* Whether she is active on twitter (boolean)\n",
    "* Whether she has a PhD (boolean)\n",
    "* Interviewed well? (boolean)\n",
    "\n",
    "The classification for this dataset defines whether or not the job candidate interviewed well (True) or not (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [\n",
    "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'no'},   False),\n",
    "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'yes'},  False),\n",
    "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'no'},     True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'no'},  True),\n",
    "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'yes'},    False),\n",
    "        ({'level':'Mid','lang':'R','tweets':'yes','phd':'yes'},        True),\n",
    "        ({'level':'Senior','lang':'Python','tweets':'no','phd':'no'}, False),\n",
    "        ({'level':'Senior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'yes','phd':'no'}, True),\n",
    "        ({'level':'Senior','lang':'Python','tweets':'yes','phd':'yes'},True),\n",
    "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'yes'},    True),\n",
    "        ({'level':'Mid','lang':'Java','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'yes'},False)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phd 0.892158928262\n",
      "level 0.693536138896\n",
      "tweets 0.788450457308\n",
      "lang 0.860131712855\n",
      "level\n"
     ]
    }
   ],
   "source": [
    "def partition_by(inputs, attribute):\n",
    "    '''\n",
    "    inputs is a list of tuple pairs: (attribute_dict, label)\n",
    "    attribute is the proposed attribute to partition by\n",
    "    returns a dictionary of attribute value: input subsets pairs\n",
    "    '''\n",
    "    subsets = {}\n",
    "    for example in inputs:\n",
    "        attribute_value = example[0][attribute]\n",
    "        if attribute_value in subsets:\n",
    "            subsets[attribute_value].append(example)\n",
    "        else: # add this attribute_value to the dict\n",
    "            subsets[attribute_value] = [example]\n",
    "    return subsets\n",
    "\n",
    "def partition_entropy_by(inputs, attribute):\n",
    "    '''\n",
    "    compute the partition\n",
    "    compute the entropy of the partition\n",
    "    '''\n",
    "    subsets = partition_by(inputs, attribute)\n",
    "    entropies = compute_partition_entropy(subsets.values())\n",
    "    return entropies\n",
    "\n",
    "def find_min_entropy_partition(inputs, attributes=None):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    if attributes is None:\n",
    "        attributes = list(inputs[0][0].keys())\n",
    "    partition_entropies = []\n",
    "    for attribute in attributes:\n",
    "        partition_entropy = partition_entropy_by(inputs, attribute)\n",
    "        print(attribute, partition_entropy)\n",
    "        partition_entropies.append(partition_entropy)\n",
    "    min_index = np.argmin(partition_entropies)\n",
    "    return attributes[min_index]\n",
    "        \n",
    "attribute = find_min_entropy_partition(inputs)\n",
    "print(attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the entropy values for splitting on each possible attribute. The attribute `level` gives the lowest entropy so this will be our first attribute to split on in our decision tree. Now comes the part of actually building the tree. We are going to follow Joel Grus' implementation and define a *tree* to be one of the following:\n",
    "* `True` (leaf node): positive classification\n",
    "* `False` (leaf node): negative classification\n",
    "* `(attribute, subtree_dict)` (decision node): a tuple that classifies an example by `subtree_dict` using `attribute`\n",
    "\n",
    "To handle a missing (or unexpected) attribute value, we'll add a `None` key that just predicts the most common label (not a good idea if `None` is a valid attribute value in the dataset, which isn't the case for our job candidate dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phd 0.892158928262\n",
      "level 0.693536138896\n",
      "tweets 0.788450457308\n",
      "lang 0.860131712855\n",
      "phd 0.950977500433\n",
      "level 0.970950594455\n",
      "tweets 0.0\n",
      "lang 0.4\n",
      "phd 0.0\n",
      "level 0.970950594455\n",
      "tweets 0.950977500433\n",
      "lang 0.950977500433\n",
      "('level', {None: True, 'Senior': ('tweets', {'no': False, None: False, 'yes': True}), 'Junior': ('phd', {'no': True, None: True, 'yes': False}), 'Mid': True})\n"
     ]
    }
   ],
   "source": [
    "def build_tree(inputs, split_candidates=None):\n",
    "    '''\n",
    "    implements the ID3 algorithm to build a decision tree\n",
    "    '''\n",
    "    if split_candidates is None:\n",
    "        # this is the first pass\n",
    "        split_candidates = list(inputs[0][0].keys())\n",
    "        \n",
    "    num_examples = len(inputs)\n",
    "    # count Trues and Falses in the examples\n",
    "    num_trues = len([label for attributes, label in inputs if label == True])\n",
    "    num_falses = num_examples - num_trues\n",
    "    \n",
    "    # part (1) in the ID3 algorithm -> all same class label\n",
    "    if num_trues == 0: # no trues, this is a False leaf node\n",
    "        return False\n",
    "    if num_falses == 0: # no falses, this is a True leaf node\n",
    "        return True\n",
    "    \n",
    "    # part (2) in the ID3 algorithm -> list of attributes is empty -> leaf node with majority class label\n",
    "    if not split_candidates: \n",
    "        return num_trues >= num_falses\n",
    "    \n",
    "    # part (3) in ID3 algorithm -> split on best attribute\n",
    "    split_attribute = find_min_entropy_partition(inputs, split_candidates)\n",
    "    partitions = partition_by(inputs, split_attribute)\n",
    "    new_split_candidates = split_candidates.remove(split_attribute)\n",
    "    \n",
    "    # recursively build the subtrees\n",
    "    subtrees = {}\n",
    "    for attribute_value, subset in partitions.items():\n",
    "        subtrees[attribute_value] = build_tree(subset, new_split_candidates)\n",
    "        \n",
    "    # missing (or unexpected) attribute value\n",
    "    subtrees[None] = num_trues > num_falses\n",
    "    \n",
    "    return (split_attribute, subtrees)\n",
    "\n",
    "tree = build_tree(inputs)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/gsprint23/cpts215/master/lessons/figures/job_candidate_tree.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Unseen Data\n",
    "For new, unseen examples, we want to use our tree to make classifications! Since we already built our tree, all we need to do is route our new example through the tree, following different branches based on the example's attribute values. Eventually we will reach a leaf node and that will be our classification for the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def classify(tree, new_example):\n",
    "    '''\n",
    "    classify new_example using decision tree\n",
    "    '''\n",
    "    # leaf node, return value\n",
    "    if tree in [True, False]:\n",
    "        return tree\n",
    "\n",
    "    # decision node, unpack the attribute to split on and subtrees\n",
    "    attribute, subtree_dict = tree\n",
    "    \n",
    "    subtree_key = new_example.get(attribute) # get return None if attribute not in new_example dict\n",
    "    if subtree_key not in subtree_dict:\n",
    "        subtree_key = None # use None subtree if no subtree for key\n",
    "        \n",
    "    subtree = subtree_dict[subtree_key]\n",
    "    label = classify(subtree, new_example)\n",
    "    return label\n",
    "\n",
    "ex1 = {\"level\": \"Junior\", \"lang\": \"Java\", \"tweets\": \"yes\", \"phd\": \"no\"} # True\n",
    "ex2 = {\"level\": \"Junior\", \"lang\": \"Java\", \"tweets\": \"yes\", \"phd\": \"yes\"} # False\n",
    "ex3 = {\"level\": \"Intern\"} # True\n",
    "ex4 = {\"level\": \"Senior\"} # False\n",
    "\n",
    "print(classify(tree, ex1))\n",
    "print(classify(tree, ex2))\n",
    "print(classify(tree, ex3))\n",
    "print(classify(tree, ex4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Summary\n",
    "We only scratched the surface on decision trees! In future data analytics and machine learning classes you will cover them in more detail. Some topics related to decision trees that are important to note:\n",
    "* We do not want to construct a tree that is *overfit* for our training dataset and cannot generalize well to new examples. One approach to prevent overfitting is called [pruning](https://en.wikipedia.org/wiki/Pruning_(decision_trees)).\n",
    "* [Random forests](https://en.wikipedia.org/wiki/Random_forest) is a [*ensemble* machine learning algorithm](https://en.wikipedia.org/wiki/Ensemble_learning) that trains a collection (forest) of decision trees. Random Forests help address overfitting and can boost classification accuracy.\n",
    "* Evaluating the accuracy of a classifier is important. To do this, we want a large, representative dataset that we can split into a training set, a validation set (for tuning parameters, such as pruning), and a testing set (to actually measure the accuracy).\n",
    "* Your initial set of attributes can greatly impact the performance of a decision tree. We don't want to include attributes that have a high number of possible values (e.g. approaching the number of examples in the dataset) and we don't want to include attributes that noisy."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
