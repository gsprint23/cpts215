{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CptS 215 Data Analytics Systems and Algorithms](https://github.com/gsprint23/cpts215)\n",
    "[Washington State University](https://wsu.edu)\n",
    "\n",
    "[Gina Sprint](http://eecs.wsu.edu/~gsprint/)\n",
    "# Decision Trees\n",
    "\n",
    "Learner objectives for this lesson:\n",
    "* Understand what machine learning is\n",
    "* Understand the difference between supervised vs. unsupervised machine learning\n",
    "* Understand a decision tree\n",
    "* Understand entropy\n",
    "* Implement the ID3 tree building algorithm\n",
    "\n",
    "\n",
    "## Acknowledgments\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* [Data Science from Scratch](https://www.amazon.com/Data-Science-Scratch-Principles-Python/dp/149190142X/ref=sr_1_1?ie=UTF8&qid=1491521130&sr=8-1&keywords=joel+grus) by Joel Grus\n",
    "* [Data Mining](https://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0123748569/ref=sr_1_2?ie=UTF8&qid=1491521294&sr=8-2&keywords=data+mining+ian) by Ian Witten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "At a high level, machine learning is building and using models that are learned from data. Machine learning is a subset of artificial intelligence, and it greatly overlaps with data mining. Let's see the \"unofficial\" definitions for these areas from Wikipedia:\n",
    "* [Data mining](https://en.wikipedia.org/wiki/Data_mining): The computational process of discovering patterns in large data sets involving methods at the intersection of artificial intelligence, machine learning, statistics, and database systems. It is an interdisciplinary subfield of computer science. The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use.\n",
    "    * Take away point: Discovering and using patterns in data\n",
    "* [Artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence): The study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of success at some goal. Colloquially, the term \"artificial intelligence\" is applied when a machine mimics \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\" (known as Machine Learning).\n",
    "    * Take away point: Implementing human-cognition on a machine\n",
    "* [Machine learning](https://en.wikipedia.org/wiki/Machine_learning): The subfield of computer science that, according to Arthur Samuel in 1959, gives \"computers the ability to learn without being explicitly programmed.\" Evolved from the study of pattern recognition and computational learning theory in artificial intelligence, machine learning explores the study and construction of algorithms that can learn from and make predictions on data â€“ such algorithms overcome following strictly static program instructions by making data driven predictions or decisions, through building a model from sample inputs.\n",
    "    * Take away point: Learning from and making predictions on data\n",
    "    \n",
    "At WSU, we have full classes in each one of these areas. In CptS 215, we are learning the data structures that underlie many of the algorithms in these fields. For example, we have already learning about the $k$-means clustering algorithm and how it can be used to group data examples together. This algorithm is an example of an *unsupervised* machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "Supervised learning requires labeled training data from a \"supervisor.\" Such labels are considered the ground-truth for describing the data. The label comes from a knowledgeable expert and can be used to learn what information describes different labels.\n",
    "\n",
    "Supervised learning is typically composed of training and testing. We will train a machine (AKA a student, learner, mathematical model) to learn a concept. Then we will test the machine's learned concept by applying their knowledge.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png\" width=\"650\">\n",
    "(image from [https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png](https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png))\n",
    "\n",
    "### Training\n",
    "As an example, suppose you are trying to teach someone (say a student) who has no notion of a cat or dog, the concept of cat vs. dog. You might first show the student some pictures of cats and say, \"these are cats\". Then you might show the person some pictures of dogs and say, \"these are dogs\". The set of cat and dog images is called the *training set*, a set of examples (also known as features). For example, consider the following cat vs. dog training set:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/cpts215/master/lessons/figures/cat_dog_training.png\" width=\"500\"/>\n",
    "\n",
    "The student is going to look at different attributes of the image to try to learn a model of cat and a model of a dog. In doing so, the student will identify some aspects (or *features*) of the examples that distinguish a cat vs a dog. The features might include:\n",
    "\n",
    "|Feature|Cat value|Dog value|\n",
    "|-|-|-|\n",
    "|Tongue out|No|Yes|\n",
    "|Fur color|Light|Dark|\n",
    "|Ears up|Yes|No|\n",
    "\n",
    "What other features did you come up with?\n",
    "\n",
    "#### Building a Model\n",
    "A model to represent cat vs. dog based on these features might be rule-based:\n",
    "\n",
    ">if tongue is out and the fur is dark and the ears are down then this is a dog\n",
    "\n",
    "We will see later how we can use a tree as a model to represent a classification such as dog vs. cat!\n",
    "\n",
    "### Testing\n",
    "Now, suppose we want to apply the student's learned conception of dog vs. cat by providing the student with a new, unseen example:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/cpts215/master/lessons/figures/cat_or_dog.png\" width=\"150\"/>\n",
    "\n",
    "Based on the above features, this image has the tongue out (dog), light fur color (cat), and ears up (cat). Thus our student would likely classify this image as a cat. But wait! We (the expert supervisors) know this is a dog (a puppy, but a dog none the less). Our training set didn't include any images that were as borderline cat/dog as this testing example. As you can see, the examples that comprise your training set and the features that are utilized greatly impact the accuracy of the learner, and consequently the model that is built. \n",
    "\n",
    "### Supervised Models\n",
    "The field of supervised machine learning is quite vast. We are only going to just scratch the surface enough to implement a machine learning model called a decision tree. You will learn more about machine learning in your future data analytics classes :) For the inquiring minds, here are a few of the most popular supervised learning models with links to find out more information about them:\n",
    "* [Decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "* [Regression analysis](https://en.wikipedia.org/wiki/Regression_analysis)\n",
    "* [Support vector machines](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "* [Naive Bayes'](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "* [Neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network)\n",
    "\n",
    "There is also a great [machine learning course](https://www.coursera.org/learn/machine-learning?utm_source=gg&utm_medium=sem&campaignid=685340575&adgroupid=32639001781&device=c&keyword=machine%20learning%20tutorial&matchtype=e&network=g&devicemodel=&adpostion=1t2&creativeid=176448312903&hide_mobile_promo&gclid=Cj0KEQjwxbDIBRCL99Wls-nLicoBEiQAWroh6s653zQm-VeTv7AfhYWijaykf_XRX-NEbfMaZ18_lHQaAhlM8P8HAQ) by Andrew Ng on Coursera. Check it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Aside: Unsupervised Learning\n",
    "Unsupervised learning does not require labeled training data. Information learned from the examples is data-driven and includes the process of discovering and describing patterns in the data. \n",
    "\n",
    "For example, to apply unsupervised learning to our cat vs. dog example, we would not try to \"train\" our student to learn the notion of \"cat\" or \"dog\". Instead, we would have our student look for patterns in the data, or perhaps a natural grouping. \n",
    "\n",
    "Here are our cat-dog training examples sorted in order based on the feature fur color:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/cpts215/master/lessons/figures/cat_dog_fur_ordering.png\" width=\"500\"/>\n",
    "\n",
    "We could apply a clustering algorithm, such as $k$-means clustering that we have already learned and implemented, to the data to reveal two natural groups in the data ($k = 2$):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/cpts215/master/lessons/figures/cat_dog_grouping.png\" width=\"500\"/>\n",
    "\n",
    "Note that these two groups, blue and red, are not representative of cat and dog, since we have no cat/dog labels!\n",
    "\n",
    "Now, upon seeing a new example, we can determine the new examples membership to either the blue group or the red group:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/cpts215/master/lessons/figures/cat_dog_membership.png\" width=\"500\"/>\n",
    "\n",
    "Like supervised machine learning, there are several unsupervised machine learning algorithms. Here are a few of the most popular unsupervised learning models with links to find out more information about them:\n",
    "* [Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) such as k-means, hierarchical, and mixture models\n",
    "    * Note: we will cover hierarchical clustering when we cover trees!\n",
    "* [Anomaly (outlier) detection](https://en.wikipedia.org/wiki/Anomaly_detection)\n",
    "* [Artificial neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network#Unsupervised_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "A decision tree uses a tree to represent a number of possible *decision paths* and an outcome for each path. Decision trees are a popular machine learning model because they are easy to read and interpret, the decision path for unseen data is transparent, and when used in more advanced machine learning algorithms (random forests, ensemble learning, etc.) they form quite powerful predictive models. Decision trees can be trained to predict categorical outcomes (classification trees) or numeric outcomes (regression trees). We are going to focus on *binary* classification trees, trees that predict one of two possible categorical outcomes. For example, predicting \"hire\" or \"no hire\" for potential job candidates is an example of a binary classification problem.\n",
    "\n",
    "Non-leaf nodes (decision nodes) in a decision tree involve testing a particular feature, or attribute, typically against a constant value. Leaf nodes in a decision tree represent a classification, a set of classifications, or a probability distribution over all possible classifications that applies to all instances that reach the leaf.\n",
    "\n",
    "Let's take a look at an example. Suppose we have built a tree to classify whether a passenger aboard the [Titanic](https://en.wikipedia.org/wiki/RMS_Titanic) survived the shipwreck or not:\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png\" width=\"400\"/>\n",
    "\n",
    "(image from [https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png))\n",
    "\n",
    "Where \"sex\" is the gender of the passenger, \"age\" is the age of the passenger in years (fractional if age is less than one), and \"sibsp\" is the number of siblings/spouses aboard the vessel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through a few examples of making a prediction for different passengers using this tree. To classify an example passenger as \"survived\" or \"died\" the example is routed down the tree according to the values of its attributes tested in successive nodes. When a leaf node is reached the example is classified based on the class assigned to the leaf.\n",
    "\n",
    "|sex|age|sibsp|prediction|\n",
    "|-|-|-|-|\n",
    "|female|30|0|survived|\n",
    "|male|45|2|died|\n",
    "|male|8|0|survived|\n",
    "|male|6|3|died|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Tree Implementation\n",
    "To implement a classification tree, we are going to have to decide what questions to ask (and in what order) to partition our dataset into a decision tree structure. To do this, we need to find out which features are the most informative. A feature that clearly divides the examples is a good feature to use in our tree. \n",
    "\n",
    "### Entropy\n",
    "Let's first discuss the notion of *entropy*. Entropy is going to be our measure of how much uncertainty is contained in the data. If all examples in a dataset belong to the same class, then there is no uncertainty about which class these examples belong to and therefore low entropy. If the examples in a dataset are evenly spread across different classes, then there is a high amount of uncertainty and therefore high entropy.\n",
    "\n",
    "(notation adopted from Joel Grus)\n",
    "\n",
    "To code up a function to compute the entropy of a data set, we need a mathematical definition of entropy. Let $S$ by a dataset contain examples that each belong to one of a finite number ($n$) of classes $C_{1},...,C_{n}$. If $P_{i}$ is the proportion of data labeled as class $C_{i}$, then entropy $H$ of the dataset $S$ is defined as:\n",
    "\n",
    "$$H(S) = -p_{1} log_{2} p_{1} - ... - p_{n} log_{2} p_{n}$$\n",
    "\n",
    "such that $0 log 0 = 0$. Each term $-p_{i} log_{2} p_{i}$ is close to zero when $p_{i}$ is either close to zero or close to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x53b2bf58d0>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEQCAYAAABMXyhMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x53b0a20400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "p = np.arange(0.01, 1.01, 0.01)\n",
    "plogp = -p * np.log2(p)\n",
    "plt.xlabel(\"$p$\")\n",
    "plt.ylabel(\"$p log_{2}(p)$\")\n",
    "plt.plot(p, plogp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summation of $H(s)$ will be small when each $p_{i}$ is close to zero or close to one (most examples belong to a single class) and will be large when each $p_{i}$ is close to ~0.5 (the examples are spread across multiple classes). Let's write the code to represent this mathematical behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1  0.1  0.8] 0.921928094887\n",
      "[ 0.3  0.4  0.3] 1.57095059445\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_entropy(class_probabilities):\n",
    "    '''\n",
    "    class_probabilities is a list of class probabilities\n",
    "    '''\n",
    "    terms = [-pi * np.log2(pi) for pi in class_probabilities if pi] # ignore zero probabilities\n",
    "    H = np.sum(terms)\n",
    "    return H\n",
    "\n",
    "def compute_class_probabilities(instance_labels):\n",
    "    '''\n",
    "    instance_labels is a list of each examples' class label\n",
    "    '''\n",
    "    num_examples = len(instance_labels)\n",
    "    counts = list(Counter(instance_labels).values())\n",
    "    probabilities = np.array(counts) / num_examples\n",
    "    return probabilities\n",
    "\n",
    "def compute_subset_entropy(subset):\n",
    "    '''\n",
    "    subset is a list of instances as two-item tuples (attributes, label)\n",
    "    '''\n",
    "    labels = [label for _, label in subset]\n",
    "    probabilities = compute_class_probabilities(labels)\n",
    "    entropy = compute_entropy(probabilities)\n",
    "    return entropy\n",
    "    \n",
    "# all in one class -> low uncertainty -> low entropy\n",
    "x = np.array([5, 5, 5, 5, 5, 1, 5, 2, 5, 5])\n",
    "cps = compute_class_probabilities(x)\n",
    "entropy = compute_entropy(cps)\n",
    "print(cps, entropy)\n",
    "\n",
    "# even spread across classes -> high uncertainty -> high entropy\n",
    "x = np.array([5, 5, 5, 1, 1, 1, 2, 2, 2, 2])\n",
    "cps = compute_class_probabilities(x)\n",
    "entropy = compute_entropy(cps)\n",
    "print(cps, entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to apply our measurement of entropy to build our tree. As we construct our tree, we are going to need to figure out what decision to make at our decision nodes. To do this, we are going to partition our data set $S$ into subsets $S_{1},...,S_{m}$ containing proportions $q_{1},...,q_{m}$ of the data. We can then compute the entropy of the partition as the weighted sum of each subset's entropy:\n",
    "\n",
    "$$H = -q_{1} H(S_{1}) + ... + q_{m} H(S_{m})$$\n",
    "\n",
    "Ideally, we want a partition to have low entropy if it splits the data into subsets that have low entropy and high entropy if it splits the data insto subsets that are large and have high entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_partition_entropy(subsets):\n",
    "    '''\n",
    "    subsets is a list of class label lists\n",
    "    '''\n",
    "    num_examples = np.sum([len(s) for s in subsets])\n",
    "    entropies = [(len(s) / num_examples) * compute_subset_entropy(s) for s in subsets]\n",
    "    partition_entropy = np.sum(entropies)\n",
    "    return partition_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Algorithm\n",
    "Next, we need to add our infrastructure to read in the data, build a tree, and make classifications on unseen data. To build a tree, we are going to implement the ID3 algorithm. Starting with the entire dataset and all attributes, we are going to follow the following process to build a tree (from Joel Grus' text):\n",
    "1. If the data all have the same label, then create a leaf node that predicts that label and then stop.\n",
    "1. If the list of attributes is empty (i.e. there are no more possible questions to ask), then create a leaf node that predicts the most common classification label and then stop.\n",
    "1. Otherwise, try partition the data by each of the attributes.\n",
    "    1. Choose the partition with the lowest partition entropy.\n",
    "    1. Add a decision node based on the chosen attribute.\n",
    "    1. Recur on each partitioned subset using the remaining attributes.\n",
    "    \n",
    "> This is what's known as a \"greedy\" algorithm because, at each step, it chooses the most immediately best option. Given a dataset, there may be a more optimal tree with a worse-looking first move. If so, this algorithm won't find it. Nonetheless, it is relatively easy to understand and implement, which makes it a good place to being exploring decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Dataset\n",
    "To implement the ID3 algorithm step by step, we will use Joel Grus' example dataset. In this dataset, each instance example is an attribute list describing a job candidate:\n",
    "* Level of expertise (string)\n",
    "* Preferred language (string)\n",
    "* Whether she is active on twitter (boolean)\n",
    "* Whether she has a PhD (boolean)\n",
    "* Interviewed well? (boolean)\n",
    "\n",
    "The classification for this dataset defines whether or not the job candidate interviewed well (True) or not (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [\n",
    "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'no'},   False),\n",
    "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'yes'},  False),\n",
    "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'no'},     True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'no'},  True),\n",
    "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'yes'},    False),\n",
    "        ({'level':'Mid','lang':'R','tweets':'yes','phd':'yes'},        True),\n",
    "        ({'level':'Senior','lang':'Python','tweets':'no','phd':'no'}, False),\n",
    "        ({'level':'Senior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'yes','phd':'no'}, True),\n",
    "        ({'level':'Senior','lang':'Python','tweets':'yes','phd':'yes'},True),\n",
    "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'yes'},    True),\n",
    "        ({'level':'Mid','lang':'Java','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'yes'},False)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phd 0.892158928262\n",
      "lang 0.860131712855\n",
      "tweets 0.788450457308\n",
      "level 0.693536138896\n",
      "level\n"
     ]
    }
   ],
   "source": [
    "def partition_by(inputs, attribute):\n",
    "    '''\n",
    "    inputs is a list of tuple pairs: (attribute_dict, label)\n",
    "    attribute is the proposed attribute to partition by\n",
    "    returns a dictionary of attribute value: input subsets pairs\n",
    "    '''\n",
    "    subsets = {}\n",
    "    for example in inputs:\n",
    "        attribute_value = example[0][attribute]\n",
    "        if attribute_value in subsets:\n",
    "            subsets[attribute_value].append(example)\n",
    "        else: # add this attribute_value to the dict\n",
    "            subsets[attribute_value] = [example]\n",
    "    return subsets\n",
    "\n",
    "def partition_entropy_by(inputs, attribute):\n",
    "    '''\n",
    "    compute the partition\n",
    "    compute the entropy of the partition\n",
    "    '''\n",
    "    subsets = partition_by(inputs, attribute)\n",
    "    entropies = compute_partition_entropy(subsets.values())\n",
    "    return entropies\n",
    "\n",
    "def find_min_entropy_partition(inputs, attributes=None):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    if attributes is None:\n",
    "        attributes = list(inputs[0][0].keys())\n",
    "    partition_entropies = []\n",
    "    for attribute in attributes:\n",
    "        partition_entropy = partition_entropy_by(inputs, attribute)\n",
    "        print(attribute, partition_entropy)\n",
    "        partition_entropies.append(partition_entropy)\n",
    "    min_index = np.argmin(partition_entropies)\n",
    "    return attributes[min_index]\n",
    "        \n",
    "attribute = find_min_entropy_partition(inputs)\n",
    "print(attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the entropy values for splitting on each possible attribute. The attribute `level` gives the lowest entropy so this will be our first attribute to split on in our decision tree. Now comes the part of actually building the tree. We are going to follow Joel Grus' implementation and define a *tree* to be one of the following:\n",
    "* `True` (leaf node): positive classification\n",
    "* `False` (leaf node): negative classification\n",
    "* `(attribute, subtree_dict)` (decision node): a tuple that classifies an example by `subtree_dict` using `attribute`\n",
    "\n",
    "To handle a missing (or unexpected) attribute value, we'll add a `None` key that just predicts the most common label (not a good idea if `None` is a valid attribute value in the dataset, which isn't the case for our job candidate dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phd 0.892158928262\n",
      "lang 0.860131712855\n",
      "tweets 0.788450457308\n",
      "level 0.693536138896\n",
      "phd 0.950977500433\n",
      "lang 0.4\n",
      "tweets 0.0\n",
      "level 0.970950594455\n",
      "phd 0.0\n",
      "lang 0.950977500433\n",
      "tweets 0.950977500433\n",
      "level 0.970950594455\n",
      "('level', {'Senior': ('tweets', {'no': False, None: False, 'yes': True}), 'Mid': True, None: True, 'Junior': ('phd', {'no': True, None: True, 'yes': False})})\n"
     ]
    }
   ],
   "source": [
    "def build_tree(inputs, split_candidates=None):\n",
    "    '''\n",
    "    implements the ID3 algorithm to build a decision tree\n",
    "    '''\n",
    "    if split_candidates is None:\n",
    "        # this is the first pass\n",
    "        split_candidates = list(inputs[0][0].keys())\n",
    "        \n",
    "    num_examples = len(inputs)\n",
    "    # count Trues and Falses in the examples\n",
    "    num_trues = len([label for attributes, label in inputs if label == True])\n",
    "    num_falses = num_examples - num_trues\n",
    "    \n",
    "    # part (1) in the ID3 algorithm -> all same class label\n",
    "    if num_trues == 0: # no trues, this is a False leaf node\n",
    "        return False\n",
    "    if num_falses == 0: # no falses, this is a True leaf node\n",
    "        return True\n",
    "    \n",
    "    # part (2) in the ID3 algorithm -> list of attributes is empty -> leaf node with majority class label\n",
    "    if not split_candidates: \n",
    "        return num_trues >= num_falses\n",
    "    \n",
    "    # part (3) in ID3 algorithm -> split on best attribute\n",
    "    split_attribute = find_min_entropy_partition(inputs, split_candidates)\n",
    "    partitions = partition_by(inputs, split_attribute)\n",
    "    new_split_candidates = split_candidates.remove(split_attribute)\n",
    "    \n",
    "    # recursively build the subtrees\n",
    "    subtrees = {}\n",
    "    for attribute_value, subset in partitions.items():\n",
    "        subtrees[attribute_value] = build_tree(subset, new_split_candidates)\n",
    "        \n",
    "    # missing (or unexpected) attribute value\n",
    "    subtrees[None] = num_trues > num_falses\n",
    "    \n",
    "    return (split_attribute, subtrees)\n",
    "\n",
    "tree = build_tree(inputs)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/gsprint23/cpts215/master/lessons/figures/job_candidate_tree.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Unseen Data\n",
    "For new, unseen examples, we want to use our tree to make classifications! Since we already built our tree, all we need to do is route our new example through the tree, following different branches based on the example's attribute values. Eventually we will reach a leaf node and that will be our classification for the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def classify(tree, new_example):\n",
    "    '''\n",
    "    classify new_example using decision tree\n",
    "    '''\n",
    "    # leaf node, return value\n",
    "    if tree in [True, False]:\n",
    "        return tree\n",
    "\n",
    "    # decision node, unpack the attribute to split on and subtrees\n",
    "    attribute, subtree_dict = tree\n",
    "    \n",
    "    subtree_key = new_example.get(attribute) # get return None if attribute not in new_example dict\n",
    "    if subtree_key not in subtree_dict:\n",
    "        subtree_key = None # use None subtree if no subtree for key\n",
    "        \n",
    "    subtree = subtree_dict[subtree_key]\n",
    "    label = classify(subtree, new_example)\n",
    "    return label\n",
    "\n",
    "ex1 = {\"level\": \"Junior\", \"lang\": \"Java\", \"tweets\": \"yes\", \"phd\": \"no\"} # True\n",
    "ex2 = {\"level\": \"Junior\", \"lang\": \"Java\", \"tweets\": \"yes\", \"phd\": \"yes\"} # False\n",
    "ex3 = {\"level\": \"Intern\"} # True\n",
    "ex4 = {\"level\": \"Senior\"} # False\n",
    "\n",
    "print(classify(tree, ex1))\n",
    "print(classify(tree, ex2))\n",
    "print(classify(tree, ex3))\n",
    "print(classify(tree, ex4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Summary\n",
    "We only scratched the surface on decision trees! In future data analytics and machine learning classes you will cover them in more detail. Some topics related to decision trees that are important to note:\n",
    "* We do not want to construct a tree that is *overfit* for our training dataset and cannot generalize well to new examples. One approach to prevent overfitting is called [pruning](https://en.wikipedia.org/wiki/Pruning_(decision_trees)).\n",
    "* [Random forests](https://en.wikipedia.org/wiki/Random_forest) is a [*ensemble* machine learning algorithm](https://en.wikipedia.org/wiki/Ensemble_learning) that trains a collection (forest) of decision trees. Random Forests help address overfitting and can boost classification accuracy.\n",
    "* Evaluating the accuracy of a classifier is important. To do this, we want a large, representative dataset that we can split into a training set, a validation set (for tuning parameters, such as pruning), and a testing set (to actually measure the accuracy).\n",
    "* Your initial set of attributes can greatly impact the performance of a decision tree. We don't want to include attributes that have a high number of possible values (e.g. approaching the number of examples in the dataset) and we don't want to include attributes that noisy."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
